## Проекты курса Инженер данных на платформе Yandex Practicum  

| Проект                         | Технологии, инструменты, библиотеки  | Результаты |
| :-------------------- | :--------------------- |:---------------------------|:---------------------------|
| [Проверка качества данных. Витрина метрики RFM.](</01 Проверка качества данных. Витрина метрики RFM./README.md>)         | Проверить качество исходных данных (пропуски, повторы, форматы, некорректные записи)   <P><P>Создать витрины данных для RFM-классификации пользователей           | SQL, CTE, Window Functions, PostgreSQL, cloudbeaver | Расширил навыки использования оконных функций, встроенных в `PostgreSQL` механизмов проверок данных <P><P>Реализовал `Common Table Expression`, использовал `cloudbeaver`
| [Модификация DWH. Миграция в новую модель.](</02 Модификация DWH. Миграция в новую модель./README.md>) | Мигрировать данные в отдельные логические таблицы  <P><P>Создать витрину данных для новой модели данных   | SQL, Window Functions, PostgreSQL, cloudbeaver          | Освоил принципы построения `DataWareHouse`, состав и назначение слоев хранилища данных
| [Модификация ETL и витрины. Реализация идемпотентности.](</03 Модификация ETL и витрины. Реализация идемпотентности./README.md>)         | Модифицировать процессы в пайплайне, чтобы они соответствовали новым задачам бизнеса, обеспечив обратную совместимость <P><P>Создать обновленную витрину данных для исследования возвращаемости клиентов<P><P>Модифицировать ETL процесс, чтобы поддерживалась идемпотентность         | AirFlow, SQL, PostgreSQL, cloudbeaver, bash, pandas, SQLAlchemy, PostgresOperator, BashOperator        | Освоил принципы реализации оркестрации, реализовал с помощью `Airflow` <P><P>Реализовал работу с `PostgreSQL` из Python через библиотеки `psycopg2`, `SQLAlchemy` <P><P>Овладел подходами к реализации идемпотентности.  
| [Проверки качества данных в ETL](</04 Проверки качества данных в ETL/README.md>)     | Определить, на каких этапах ETL процесса внедрить проверки качества данных <P><P>Разработать и внедрить проверки в ETL процесс <P><P>Создать витрину данных с результатами проверок <P><P>Составить инструкции по поддержке процессов с проверками          | AirFlow, SQL, PostgreSQL         | Расширил навыки работы с `Airflow` <P><P> Приобрел опыт работы с файлами средствами `Python` <P><P> Составил документ `RunBook` для администраторов
| [Витрина на DWH из нескольких источников](</05 Витрина на DWH из нескольких источников/README.md>)        | Усовершенствовать хранилище данных: добавить новый источник и витрину <P><P>Связать данные из нового источника с данными в хранилище <P><P>Реализовать витрину для расчётов с курьерами           | Airflow, PostgreSQL, MongoDB Compass, pendulum, Jupyter Notebook, bash, SQLAlchemy, PostgresHook    | На практике получил навык работы с NoSQL базами данных  <P><P>Упрочил навыки реализации пайпланов в `Airflow` <P><P>Расширил навыки работы с пакетом `requests`
| [Витрина на данных аналитической БД Vertica](</06 Витрина на данных аналитической БД Vertica/README.md>)         | Расширить модель данных в аналитической БД<P><P>Разработать витрину данных для оценки эффективности рекламы          | AirFlow, Yandex S3 Storage, Common Table Expression, SQL, Vertica, cloudbeaver, pandas         | Зафиксировал принципиальные отличие аналитических БД от реляционных <P><P>Приобрел опыт создания `проекций`, массовой загрузки данных в `Vertica`  
| [Spark/PySpark в Hadoop. Работа с HDFS.](</07 Spark/PySpark в Hadoop. Работа с HDFS./README.md>)       | Расширить структуру данных в Data Lake <P><P>Создать четыре витрины данных в HDFS, автоматизировать их обновление        | Hadoop, Spark, PySpark, YARN, MapReduce, Window Functions, HDFS, Airflow, SparkSubmitOperator, Parquet | Освоил работу во фреймворке `Hadoop` с `PySpark`, особенности менеджера `YARN`  <P><P>Изучил работу со `Spark`: оптимизация настроек, `data caching`, `broadcasting`, `predicate pushdown` <P><P>Изучил преимущества формата `Parquet`<P><P>Изучил технику `MapReduce`   
| [Обработка потоковых данных](</08 Обработка потоковых данных>)         | Создать сервис потоковой обработки данных, расширяющий возможности онлайн приложения по доставке еды: обогащение входной информации данными из БД, отправка сообщений в выходной поток          | Kafka, PySpark, AirFlow, kcat, Jupyter Notebook, SQL, PostgreSQL, Spark Streaming          |
| [Облачные сервисы Yandex Cloud](<09 Облачные сервисы Yandex Cloud>)        | Создать на платформе Yandex Cloud сервисы, которые реализуют ETL процесс <P><P>Визуализировать данные из новой витрины в дашборде в Datalense          | Yandex Cloud Services, Datalense, Kubernetes, kubectl, Kafka, kcat, confluent_kafka, flask, Docker Compose, Helm, Redis         |  Получил навык развертывания в сервисов в облаке `Yandex Cloud` <P><P>Создал и настроил контейнеры и оркестратор на примере `Docker Compose` и `Kubernetes` <P><P> Получил практический навык работы с очередями на примере `Kafka`
| [Объединение потоков данных. Витрина аналитики.](</10 Объединение потоков данных. Витрина аналитики.>)         | Для финтех-компании, предлагающей международные банковские услуги, объединить из разных источников информацию о финансовой активность пользователей  <P><P>Подготовить информацию для аналитиков в дашборде           | Yandex S3, DWH, Vertica, boto3, Airflow, TriggerDagRunOperator, Metabase     | Спроектировал и разработал `DWH`, ETL процесс, дашборд с использованием облачных компонентов `Yandex Cloud`, аналитической БД `Vertica`, инструмента BI отчетности `Metabase` <P><P>Автоматизировал обновление метрик в дашборде с помощью `Airflow`|
---  
