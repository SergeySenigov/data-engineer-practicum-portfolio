## Data Engineer Course Projects on the Platform of Yandex Practicum

| Project                                        | Technologies, Tools, Libraries                  | Results                                                                  |
|------------------------------------------------|-------------------------------------------------|--------------------------------------------------------------------------|
| [Data quality cheks. RFM datamart.](</01 Data quality cheks. RFM datamart./README.md>)      | Verify the quality of raw data (missing values, duplicates, formats, invalid entries)<br/><br/>Create a datamart for RFM user classification                             | SQL, CTE, Window Functions, PostgreSQL, cloudbeaver                      | Expanded skills in utilizing window functions and built-in data validation mechanisms in `PostgreSQL`<br/><br/>Implemented `Common Table Expression`, utilized `cloudbeaver`
| [Modifying DWH. Migration to the new model.](</02 Modifying DWH. Migration to the new model./README.md>)              | Migrate data into separate logical tables<br/><br/>Create a datamart for the new data model                               | SQL, Window Functions, PostgreSQL, cloudbeaver                           | Grasped the principles of constructing `Data Warehouse`, its layer structure and purposes
| [Modifying ETL and datamarts. Implementing idempotency.](</03 Modifying ETL and datamarts. Implementing idempotency./README.md>)   | Modify pipeline processes to align with new business requirements, ensure backward compatibility<br/><br/>Create an updated datamart for analyzing customer retention rates<br/><br/>Modify ETL process to support idempotence | AirFlow, SQL, PostgreSQL, cloudbeaver, bash, pandas, SQLAlchemy, PostgresOperator, BashOperator | Mastered orchestration principles through `Airflow`<br/><br/>Learned how to work with `PostgreSQL` from Python using libraries like `psycopg2`, `SQLAlchemy`<br/><br/>Acquired techniques for achieving idempotent operations.
| [Data quality checks in ETL](</04 Data quality checks in ETL/README.md>)                                      | Identify where to introduce data quality checks within the ETL process<br/><br/>Design and incorporate these checks into the ETL workflow<br/><br/>Build a datamart displaying verification outcomes.<br/><br/>Prepare documentation supporting processes that include these checks | AirFlow, SQL, PostgreSQL                                                 | Improved proficiency in working with `Airflow`<br/><br/>Gained hands-on experience processing files using `Python`<br/><br/>Prepared a `RunBook` for administrators
| [Datamart in DWH based on multiple sources](</05 Datamart in DWH based on multiple sources/README.md>)           | Refine the data warehouse by introducing a new data source and creating corresponding datamarts<br/><br/>Integrate data from this new source into the existing warehouse architecture<br/><br/>Implement a datamart calculating couriers' performance                    | Airflow, PostgreSQL, MongoDB Compass, pendulum, Jupyter Notebook, bash, SQLAlchemy, PostgresHook | Acquired practical expertise working with NoSQL databases such as `MongoDB`<br/><br/>Enhanced my understanding of designing pipelines in `Airflow`<br/><br/>Expanded my ability to utilize the `requests` library
| [Datamart based on Analytical Database Vertica](</06 Datamart based on Analytical Database Vertica/README.md>)    | Expand the data model in analytical database<p><p>Develop a datamart evaluating advertising effectiveness                                          | AirFlow, Yandex S3 Storage, Common Table Expression, SQL, Vertica, cloudbeaver, pandas               | Documented key differences between analytical and relational databases<p><p>Gained practical experience in creating projections and bulk loading data into `Vertica`
| [Working with PySpark in Hadoop. Working with HDFS.](</07 Working with PySpark in Hadoop. Working with HDFS./README.md>) | Extend the data structure in Data Lake<p><p>Create four datamarts in HDFS, automating their updates                                           | Hadoop, Spark, PySpark, YARN, MapReduce, Window Functions, HDFS, Airflow, SparkSubmitOperator, Parquet | Became familiar with operating within the `Hadoop` framework using `PySpark`, understanding nuances of `YARN` manager<p><p>Explored `Spark` internals including optimization settings, `data caching`, `broadcasting`, `predicate pushdown`<p><p>Studied advantages of `Parquet` format<p><p>Examined `MapReduce` technique
| [Processing stream data with Spark](</08 Processing stream data with Spark/README.md>)                                             | Develop a streaming data service extending capabilities of online food delivery application: enrich input information with DB data, send messages to output stream                                               | Kafka, PySpark, AirFlow, kcat, Jupyter Notebook, SQL, PostgreSQL, Spark Streaming                   |
| [Cloud services](</09 Cloud services/README.md>)                                    | On the platform of Yandex Cloud create services implementing ETL process<p><p>Visualize data from new datamarts in dashboards in Datalense                                                    | Yandex Cloud Services, Datalense, Kubernetes, kubectl, Kafka, kcat, confluent_kafka, flask, Docker Compose, Helm, Redis | Obtained practical experience deploying services in `Yandex Cloud`<p><p>Designed and configured containers and orchestrators exemplified by `Docker Compose` and `Kubernetes`<p><p>Acquired practical knowledge about queue systems demonstrated by `Kafka`
| [Combining data streams. Analytics datamart.](</10 Combining data streams. Analytics datamart./README.md>)     | For a fintech company offering international banking services, combine financial activity data from different sources<p><p>Prepare this information for analysts in dashboard form                                              | Yandex S3, DWH, Vertica, boto3, Airflow, TriggerDagRunOperator, Metabase                              | Designed and developed a `DWH`, ETL process, and dashboard leveraging cloud components from `Yandex Cloud`, analytical database `Vertica`, and BI reporting tool `Metabase`<p><p>Automated metric updates in dashboards using `Airflow`
